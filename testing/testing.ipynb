{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we write more and more code, especially bits that start to depend on each other in complicated ways, its easy for bugs and unexpected behavior to creep in. Or, as versions of various dependencies get updated from year to year, some code that used to behave one way might stop working.\n",
    "\n",
    "To help solve these problems, we can write simple tests for our code. When we make changes to the code package, we can run the tests to make sure we haven't broken anything and the code's doing what we want. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## How do we test code with `pytest`? \n",
    "Here's a quick overview of what running tests might look like:\n",
    "\n",
    "1. Write a snippet of code that should work if your code is working and (ideally) raise an error if not. \n",
    "2. Put that code in a function that has the string `test` in its name, stored in a `.py` file with `test` in its name inside the `exoatlas/tests` directory. \n",
    "3. From the main repository directory, from the command line run `pytest`. This will search for all your test functions, run them all, and give you a report about what worked and what didn't. (The [Developer Installation](../installation/#developer-installation) should have automatically installed `pytest` for you.)\n",
    "\n",
    "If one or more of your tests break, you either need to fix something in your code or in your test function. If all of your tests pass, your should celebrate and feel very pleased with yourself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What are useful tests?\n",
    "\n",
    "The most useful tests are the ones you've actually written! It's OK if your automatic tests don't comprehensively cover every single possible way that your code might goof up; often a code's error might be a fundamental conceptual misunderstanding that can only be caught by a clever and cautious human noticing something amiss in a plot. \n",
    "\n",
    "In approximate order from simpler to more sophisticated, tests might look like:\n",
    "- Does this function run on reasonable inputs? \n",
    "- Does it run, and give plausible outputs? \n",
    "- Does it run, give accurate outputs? \n",
    "- Does it run, give accurate outputs, and respond appropriately to changing inputs? \n",
    "- Does it run, give accurate outputs, respond appropriately, and raise informative errors when it should? \n",
    "\n",
    "No matter how detailed and thoughtfully you write your tests, the Universe is complicated, so they might not capture every fascinating way your code might break. Don't aim for perfection; just try to do something."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
